{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANFIS ANALYSIS AND COMPARISON WITH ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I develop a Multiclass ANFIS and compare it with ANN to predict the presence of Heart Disease in a patient.\n",
    "\n",
    "Predicted values ranges from 0 to 4. 0 being no presence of Heart Disease and 1,2,3,4 are the stages of Heart Disease, as seen in the analysis below.\n",
    "\n",
    "`Data source` : https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/\n",
    "\n",
    "`ANFIS implementation code`: https://github.com/gregorLen/AnfisTensorflow2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from Models import myanfis\n",
    "import sys\n",
    "sys.maxsize\n",
    "# KFold \n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('heart_data.csv')\n",
    "\n",
    "# read data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing ? with NaN\n",
    "for i in data.columns:\n",
    "    data[i]=data[i].replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(data['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all variables are numeric\n",
    "data = data.loc[:, data.columns != 'region'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(data['heart_disease']).plot(kind= 'bar').set_title('Target variable Class Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing observations\n",
    "import missingno\n",
    "\n",
    "missingno.bar(data, color= 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the data\n",
    "\n",
    "- we cannot drop all missing observations.\n",
    "- we thus fill in with the mean of each column, based on age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating age group variable\n",
    "# Reference: LHENNY, 2021\n",
    "# https://www.kaggle.com/code/lhenny/simple-data-cleansing\n",
    "\n",
    "#define\n",
    "def age_group (age):\n",
    "    if age <= 35:\n",
    "        return 'youth'\n",
    "    if 36 <= age <= 45:\n",
    "        return 'adult'\n",
    "    if 46 <= age <= 60:\n",
    "        return 'senior'\n",
    "    else:\n",
    "        return 'elderly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating variable 'age group'\n",
    "#apply conversion\n",
    "data['age_group'] = data['age'].apply(age_group)\n",
    "\n",
    "#order\n",
    "age_group_classes = ['youth', 'adult', 'senior', 'elderly']\n",
    "\n",
    "#convert to factor variable\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "age_group_convert = CategoricalDtype(categories=age_group_classes, ordered = True)\n",
    "data['age_group'] = data['age_group'].astype(age_group_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing observations for trestbps in each age group\n",
    "#we use mean for each age group to fill missing trestbps for each group\n",
    "\n",
    "#mean for each age group\n",
    "trestbps_mean =  data.groupby('age_group')['trestbps'].mean()\n",
    "#round to 1 d.p\n",
    "trestbps_mean = trestbps_mean.apply(lambda x: round(x,1))\n",
    "#fill trestbps for youth group\n",
    "data.loc[(data['age_group'] == 'youth'), 'trestbps'] = data.loc[(data['age_group']=='youth'),'trestbps'].fillna(trestbps_mean.loc['youth'])\n",
    "#fill trestbps for adult group\n",
    "data.loc[(data['age_group'] == 'adult'), 'trestbps'] = data.loc[(data['age_group']=='adult'),'trestbps'].fillna(trestbps_mean.loc['adult'])\n",
    "#fill trestbps for senior group\n",
    "data.loc[(data['age_group'] == 'senior'), 'trestbps'] = data.loc[(data['age_group']=='senior'),'trestbps'].fillna(trestbps_mean.loc['senior'])\n",
    "#fill trestbps for elderly group\n",
    "data.loc[(data['age_group'] == 'elderly'), 'trestbps'] = data.loc[(data['age_group']=='elderly'),'trestbps'].fillna(trestbps_mean.loc['elderly'])\n",
    "\n",
    "\n",
    "#filling missing observations for chol in each age group\n",
    "#we use mean for each age group to fill missing chol for each group\n",
    "\n",
    "#mean for each age group\n",
    "chol_mean =  data.groupby('age_group')['chol'].mean()\n",
    "#round to 1 d.p\n",
    "chol_mean = chol_mean.apply(lambda x: round(x,1))\n",
    "#fill chol for youth group\n",
    "data.loc[(data['age_group'] == 'youth'), 'chol'] = data.loc[(data['age_group']=='youth'),'chol'].fillna(chol_mean.loc['youth'])\n",
    "#fill chol for adult group\n",
    "data.loc[(data['age_group'] == 'adult'), 'chol'] = data.loc[(data['age_group']=='adult'),'chol'].fillna(chol_mean.loc['adult'])\n",
    "#fill chol for senior group\n",
    "data.loc[(data['age_group'] == 'senior'), 'chol'] = data.loc[(data['age_group']=='senior'),'chol'].fillna(chol_mean.loc['senior'])\n",
    "#fill chol for elderly group\n",
    "data.loc[(data['age_group'] == 'elderly'), 'chol'] = data.loc[(data['age_group']=='elderly'),'chol'].fillna(chol_mean.loc['elderly'])\n",
    "\n",
    "\n",
    "#filling missing observations for fbs in each age group\n",
    "#we use mean for each age group to fill missing fbs for each group\n",
    "\n",
    "#mean for each age group\n",
    "fbs_mean =  data.groupby('age_group')['fbs'].mean()\n",
    "#round to 1 d.p\n",
    "fbs_mean = fbs_mean.apply(lambda x: round(x,1))\n",
    "#fill fbs for youth group\n",
    "data.loc[(data['age_group'] == 'youth'), 'fbs'] = data.loc[(data['age_group']=='youth'),'fbs'].fillna(fbs_mean.loc['youth'])\n",
    "#fill fbs for adult group\n",
    "data.loc[(data['age_group'] == 'adult'), 'fbs'] = data.loc[(data['age_group']=='adult'),'fbs'].fillna(fbs_mean.loc['adult'])\n",
    "#fill fbs for senior group\n",
    "data.loc[(data['age_group'] == 'senior'), 'fbs'] = data.loc[(data['age_group']=='senior'),'fbs'].fillna(fbs_mean.loc['senior'])\n",
    "#fill fbs for elderly group\n",
    "data.loc[(data['age_group'] == 'elderly'), 'fbs'] = data.loc[(data['age_group']=='elderly'),'fbs'].fillna(fbs_mean.loc['elderly'])\n",
    "\n",
    "\n",
    "#filling missing observations for restecg in each age group\n",
    "#we use mean for each age group to fill missing restecg for each group\n",
    "\n",
    "#mean for each age group\n",
    "restecg_mean =  data.groupby('age_group')['restecg'].mean()\n",
    "#round to 1 d.p\n",
    "restecg_mean = restecg_mean.apply(lambda x: round(x,1))\n",
    "#fill restecg for youth group\n",
    "data.loc[(data['age_group'] == 'youth'), 'restecg'] = data.loc[(data['age_group']=='youth'),'restecg'].fillna(restecg_mean.loc['youth'])\n",
    "#fill restecg for adult group\n",
    "data.loc[(data['age_group'] == 'adult'), 'restecg'] = data.loc[(data['age_group']=='adult'),'restecg'].fillna(restecg_mean.loc['adult'])\n",
    "#fill restecg for senior group\n",
    "data.loc[(data['age_group'] == 'senior'), 'restecg'] = data.loc[(data['age_group']=='senior'),'restecg'].fillna(restecg_mean.loc['senior'])\n",
    "#fill restecg for elderly group\n",
    "data.loc[(data['age_group'] == 'elderly'), 'restecg'] = data.loc[(data['age_group']=='elderly'),'restecg'].fillna(restecg_mean.loc['elderly'])\n",
    "\n",
    "\n",
    "#filling missing observations for thalach in each age group\n",
    "#we use mean for each age group to fill missing thalach for each group\n",
    "\n",
    "#mean for each age group\n",
    "thalach_mean =  data.groupby('age_group')['thalach'].mean()\n",
    "#round to 1 d.p\n",
    "thalach_mean = thalach_mean.apply(lambda x: round(x,1))\n",
    "#fill thalach for youth group\n",
    "data.loc[(data['age_group'] == 'youth'), 'thalach'] = data.loc[(data['age_group']=='youth'),'thalach'].fillna(thalach_mean.loc['youth'])\n",
    "#fill thalach for adult group\n",
    "data.loc[(data['age_group'] == 'adult'), 'thalach'] = data.loc[(data['age_group']=='adult'),'thalach'].fillna(thalach_mean.loc['adult'])\n",
    "#fill thalach for senior group\n",
    "data.loc[(data['age_group'] == 'senior'), 'thalach'] = data.loc[(data['age_group']=='senior'),'thalach'].fillna(thalach_mean.loc['senior'])\n",
    "#fill thalach for elderly group\n",
    "data.loc[(data['age_group'] == 'elderly'), 'thalach'] = data.loc[(data['age_group']=='elderly'),'thalach'].fillna(thalach_mean.loc['elderly'])\n",
    "\n",
    "\n",
    "#filling missing observations for exang in each age group\n",
    "#we use mean for each age group to fill missing exang for each group\n",
    "\n",
    "#mean for each age group\n",
    "exang_mean =  data.groupby('age_group')['exang'].mean()\n",
    "#round to 1 d.p\n",
    "exang_mean = exang_mean.apply(lambda x: round(x,1))\n",
    "#fill exang for youth group\n",
    "data.loc[(data['age_group'] == 'youth'), 'exang'] = data.loc[(data['age_group']=='youth'),'exang'].fillna(exang_mean.loc['youth'])\n",
    "#fill exang for adult group\n",
    "data.loc[(data['age_group'] == 'adult'), 'exang'] = data.loc[(data['age_group']=='adult'),'exang'].fillna(exang_mean.loc['adult'])\n",
    "#fill exang for senior group\n",
    "data.loc[(data['age_group'] == 'senior'), 'exang'] = data.loc[(data['age_group']=='senior'),'exang'].fillna(exang_mean.loc['senior'])\n",
    "#fill exang for elderly group\n",
    "data.loc[(data['age_group'] == 'elderly'), 'exang'] = data.loc[(data['age_group']=='elderly'),'exang'].fillna(exang_mean.loc['elderly'])\n",
    "\n",
    "\n",
    "#filling missing observations for oldpeak in each age group\n",
    "#we use mean for each age group to fill missing oldpeak for each group\n",
    "\n",
    "#mean for each age group\n",
    "oldpeak_mean =  data.groupby('age_group')['oldpeak'].mean()\n",
    "#round to 1 d.p\n",
    "oldpeak_mean = oldpeak_mean.apply(lambda x: round(x,1))\n",
    "#fill oldpeak for youth group\n",
    "data.loc[(data['age_group'] == 'youth'), 'oldpeak'] = data.loc[(data['age_group']=='youth'),'oldpeak'].fillna(oldpeak_mean.loc['youth'])\n",
    "#fill oldpeak for adult group\n",
    "data.loc[(data['age_group'] == 'adult'), 'oldpeak'] = data.loc[(data['age_group']=='adult'),'oldpeak'].fillna(oldpeak_mean.loc['adult'])\n",
    "#fill oldpeak for senior group\n",
    "data.loc[(data['age_group'] == 'senior'), 'oldpeak'] = data.loc[(data['age_group']=='senior'),'oldpeak'].fillna(oldpeak_mean.loc['senior'])\n",
    "#fill oldpeak for elderly group\n",
    "data.loc[(data['age_group'] == 'elderly'), 'oldpeak'] = data.loc[(data['age_group']=='elderly'),'oldpeak'].fillna(oldpeak_mean.loc['elderly'])\n",
    "\n",
    "\n",
    "#filling missing observations for slope in each age group\n",
    "#we use mean for each age group to fill missing slope for each group\n",
    "\n",
    "#mean for each age group\n",
    "slope_mean =  data.groupby('age_group')['slope'].mean()\n",
    "#round to 1 d.p\n",
    "slope_mean = slope_mean.apply(lambda x: round(x,1))\n",
    "#fill slope for youth group\n",
    "data.loc[(data['age_group'] == 'youth'), 'slope'] = data.loc[(data['age_group']=='youth'),'slope'].fillna(slope_mean.loc['youth'])\n",
    "#fill slope for adult group\n",
    "data.loc[(data['age_group'] == 'adult'), 'slope'] = data.loc[(data['age_group']=='adult'),'slope'].fillna(slope_mean.loc['adult'])\n",
    "#fill slope for senior group\n",
    "data.loc[(data['age_group'] == 'senior'), 'slope'] = data.loc[(data['age_group']=='senior'),'slope'].fillna(slope_mean.loc['senior'])\n",
    "#fill slope for elderly group\n",
    "data.loc[(data['age_group'] == 'elderly'), 'slope'] = data.loc[(data['age_group']=='elderly'),'slope'].fillna(slope_mean.loc['elderly'])\n",
    "\n",
    "\n",
    "#filling missing observations for ca in each age group\n",
    "#we use mean for each age group to fill missing ca for each group\n",
    "\n",
    "#mean for each age group\n",
    "ca_mean =  data.groupby('age_group')['ca'].mean()\n",
    "#round to 1 d.p\n",
    "ca_mean = ca_mean.apply(lambda x: round(x,1))\n",
    "#fill ca for youth group\n",
    "data.loc[(data['age_group'] == 'youth'), 'ca'] = data.loc[(data['age_group']=='youth'),'ca'].fillna(ca_mean.loc['youth'])\n",
    "#fill ca for adult group\n",
    "data.loc[(data['age_group'] == 'adult'), 'ca'] = data.loc[(data['age_group']=='adult'),'ca'].fillna(ca_mean.loc['adult'])\n",
    "#fill ca for senior group\n",
    "data.loc[(data['age_group'] == 'senior'), 'ca'] = data.loc[(data['age_group']=='senior'),'ca'].fillna(ca_mean.loc['senior'])\n",
    "#fill ca for elderly group\n",
    "data.loc[(data['age_group'] == 'elderly'), 'ca'] = data.loc[(data['age_group']=='elderly'),'ca'].fillna(ca_mean.loc['elderly'])\n",
    "\n",
    "\n",
    "#filling missing observations for thal in each age group\n",
    "#we use mean for each age group to fill missing thal for each group\n",
    "\n",
    "#mean for each age group\n",
    "thal_mean =  data.groupby('age_group')['thal'].mean()\n",
    "#round to 1 d.p\n",
    "thal_mean = thal_mean.apply(lambda x: round(x,1))\n",
    "#fill thal for youth group\n",
    "data.loc[(data['age_group'] == 'youth'), 'thal'] = data.loc[(data['age_group']=='youth'),'thal'].fillna(thal_mean.loc['youth'])\n",
    "#fill thal for adult group\n",
    "data.loc[(data['age_group'] == 'adult'), 'thal'] = data.loc[(data['age_group']=='adult'),'thal'].fillna(thal_mean.loc['adult'])\n",
    "#fill thal for senior group\n",
    "data.loc[(data['age_group'] == 'senior'), 'thal'] = data.loc[(data['age_group']=='senior'),'thal'].fillna(thal_mean.loc['senior'])\n",
    "#fill thal for elderly group\n",
    "data.loc[(data['age_group'] == 'elderly'), 'thal'] = data.loc[(data['age_group']=='elderly'),'thal'].fillna(thal_mean.loc['elderly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming that all variables are filled, without missing observations\n",
    "missingno.bar(data, color= 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('age_group', axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(data['heart_disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling - ANFIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''it is good to make predictions simpler, as either with heart attack or not.\n",
    "we make prediction into binary only, with the following conditions:\n",
    "1 - 4\n",
    "0 - has no attack when the value is 0 has attack, when the values are from 1 to\n",
    "\n",
    "This helps also in overfitting since class 2 to 4 are minor in count'''\n",
    "\n",
    "def binarizer(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    else: return 1\n",
    "    \n",
    "data['target'] = data['heart_disease'].apply(binarizer)\n",
    "\n",
    "pd.value_counts(data['target']).plot(kind= 'bar').set_title('Distribution of Classes in the Target variable');\n",
    "\n",
    "'''\n",
    "it is seen that the classes are now approximately evenly balanced, and the risk of overfitting is solved\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('heart_disease', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "minmaxScaler = MinMaxScaler()\n",
    "\n",
    "data2 = data.copy()\n",
    "\n",
    "# inputs = [3,4]\n",
    "# target = heart_disease\n",
    "data2['trestbps'] = minmaxScaler.fit_transform(data2[['trestbps']])\n",
    "data2['chol'] = minmaxScaler.fit_transform(data2[['chol']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the ANFIS algorithm used is from Gregor Len, link: https://github.com/gregorLen/AnfisTensorflow2.0\n",
    "The algorithm is based on Tensorflow 2.0., currently supporting two types of membership function:\n",
    " -gaussian (used in this project)\n",
    " -generalized bell\n",
    " \n",
    "Dependencies:\n",
    " -Dependencies\n",
    " -Python 3.6-3.9\n",
    " -tensorflow 2.6.0\n",
    " -numpy\n",
    " -pandas\n",
    " -sklearn\n",
    " -matplotlib\n",
    " -seaborn\n",
    " \n",
    "However, the implementation is still a work in progress since only 6 inputs are taken.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation sets\n",
    "#training\n",
    "X = data2.iloc[:-120,[3,4,5,6,8,9]]\n",
    "Y = data2.iloc[:-120,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "param = myanfis.fis_parameters(\n",
    "        n_input = 6,                # the number of Regressors\n",
    "        n_memb = 4,                 # the number of fuzzy memberships\n",
    "        batch_size = 5,           \n",
    "        memb_func = 'gaussian',      # 'gaussian' / 'gbellmf' / 'sigmoid'\n",
    "        optimizer = 'sgd',          # sgd / adam / ...\n",
    "        loss = tf.keras.losses.MeanAbsoluteError(),               # mse / mae / huber_loss / mean_absolute_percentage_error / ...\n",
    "        n_epochs = 15               # 10 / 25 / 50 / 100 / ...\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set KFold = 2 because this task is a supervised method\n",
    "kfold = KFold(n_splits=2)\n",
    "histories = []\n",
    "\n",
    "# We got the necessary indexes for our folds with kfold.split and got them as X_train, X_test, Y_train, Y_test\n",
    "# and we carried out the Fold Fold training\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    # We call ANFIS generated from Tensorflow.Keras models with the required parameters\n",
    "    fis = myanfis.ANFIS(n_input = param.n_input,\n",
    "                    n_memb = param.n_memb,\n",
    "                    batch_size = param.batch_size,\n",
    "                    memb_func = param.memb_func,\n",
    "                    name = 'myanfis' # \n",
    "                    )\n",
    "\n",
    "    # We have compiled our model with the following parameters\n",
    "    fis.model.compile(optimizer=param.optimizer,\n",
    "                      loss=param.loss,\n",
    "                      metrics=['mae', 'mse']  # ['mae', 'mse']\n",
    "                      )\n",
    "    # here we start with the training of our model\n",
    "    # then assigned the training results of the model to history\n",
    "    history = fis.fit(X_train, Y_train,\n",
    "                  epochs=param.n_epochs,\n",
    "                  batch_size=param.batch_size,\n",
    "                  validation_data = (X_test, Y_test),\n",
    "                  )\n",
    "    # Since the model will train fold fold, we kept the training results in a list called histories.\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fis.model.predict(X_test, batch_size= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "The predictions made by ANFIS are not binary (not 0 and 1, but continous), and thus,\n",
    "we  have to set the prediction to binary so we can derive the evaluation metrics\n",
    "\n",
    "'''\n",
    "y_pred_toformat = pd.DataFrame(y_pred)\n",
    "y_pred_toformat.insert(0, 'prediction_ID', range(1, 1 + len(y_pred_toformat)))\n",
    "y_pred_toformat = y_pred_toformat.rename(columns={0: \"anfis_predictions\"})\n",
    "\n",
    "\n",
    "# formatter\n",
    "def ypred_formatter(r):\n",
    "    if r <= 0:\n",
    "        return 0\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_toformat['anfis_binary_predictions'] = y_pred_toformat['anfis_predictions'].apply(ypred_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anfis_y_pred = np.array(y_pred_toformat['anfis_binary_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "plt.figure(figsize=(8,10))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "conf_mat = confusion_matrix(Y_test, anfis_y_pred)\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, anfis_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('The accuracy score of ANFIS is',accuracy_score(Y_test, anfis_y_pred)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from above, the True positive and False negative are well predicted.\n",
    "the magnitude of true negative and false positives are low (67 and 37 observations)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis.plotmfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(histories[0].history).plot()\n",
    "\n",
    "# loss = mae\n",
    "# val_loss = val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.plot(history_df.loc[:, ['loss']], \"#6daa9f\", label='Training loss')\n",
    "plt.plot(history_df.loc[:, ['val_loss']],\"#774571\", label='Validation loss')\n",
    "plt.title('ANFIS Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling - ANN (For comparison with ANFIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.131, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dropout\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(20, activation='relu', name='layer1'),\n",
    "    Dropout(0.2),\n",
    "    layers.Dense(25, activation='relu', name='layer2'),\n",
    "    Dropout(0.5),\n",
    "    layers.Dense(10, activation='relu', name='layer3'),\n",
    "    layers.Dense(2, activation='sigmoid', name='f-layer'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "model.compile(\n",
    "      loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "      optimizer = keras.optimizers.Adam(lr = 0.001),\n",
    "      metrics = ['mae', 'mse']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size = 5, epochs = 15, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_y_pred = model.predict(X_test, batch_size=5)\n",
    "\n",
    "'''\n",
    "the y predictions account for classes 0 and 1 seperately, and thus, if the values for 0 are greater than those of 1, then the class prediction is 0, otherwise, class 1.\n",
    "'''\n",
    "ann_y_pred_df = pd.DataFrame(ann_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_determiner (t):\n",
    "    if t < 0.5:\n",
    "        return 1\n",
    "    else: return 0\n",
    "    \n",
    "# apply\n",
    "ann_y_pred_df['ann_binary_predictions'] = ann_y_pred_df[0].apply(class_determiner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_y_pred = np.array(ann_y_pred_df['ann_binary_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "plt.figure(figsize=(8,10))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "conf_mat = confusion_matrix(Y_test, ann_y_pred)\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(Y_test, ann_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('The accuracy score of ANFIS is',accuracy_score(Y_test, ann_y_pred)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "surprisingly, ANN has performed better than the ANFIS classifier\n",
    "Other (and most) studies have also found out ANN to be more accurate than ANFIS.\n",
    "This is beacuse the ANFIS model is seen to overfit the data even when tuned.\n",
    "The ANN is a better classifier.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(histories[0].history).plot().set_title(\"ANN Loss, MAE, MSE, Valuation Loss\")\n",
    "\n",
    "# loss = mae\n",
    "# val_loss = val_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
